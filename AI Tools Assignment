Q1: TensorFlow vs. PyTorch
Primary Differences:

Graph Execution:
TensorFlow uses static computation graphs (define-then-run), while PyTorch uses dynamic graphs (define-by-run), allowing real-time graph modifications.

API Design:
TensorFlow has a more verbose, production-oriented API. PyTorch offers a Pythonic, intuitive interface ideal for research.

Debugging:
PyTorch integrates with Python debugging tools (pdb). TensorFlow debugging requires specialized tools like tf.debugging.

Deployment:
TensorFlow excels in production deployment (TF Serving, TFLite). PyTorch relies on TorchScript for deployment.

When to Choose:

TensorFlow: Production systems, mobile/edge deployment, TPU utilization.

PyTorch: Research prototyping, rapid experimentation, dynamic architectures (e.g., RNNs).

Q2: Jupyter Notebooks in AI
Exploratory Data Analysis (EDA):

Visualize data distributions, correlations, and anomalies interactively.

Combine code, visualizations (Matplotlib/Seaborn), and markdown explanations.

Model Prototyping & Debugging:

Iteratively test preprocessing techniques, hyperparameters, and architectures.

Immediately visualize outputs (e.g., attention maps, loss curves).

Q3: spaCy vs. Python String Operations
Contextual Understanding:
spaCy uses statistical models to resolve ambiguities (e.g., "Apple" as company vs. fruit), impossible with regex.

Linguistic Features:
Built-in tokenization, POS tagging, dependency parsing, and entity recognition.

Efficiency:
Optimized Cython code processes large text corpora faster than Python loops.

Pre-trained Models:
Leverage transformers (e.g., en_core_web_trf) for state-of-the-art accuracy.

Comparative Analysis: Scikit-learn vs. TensorFlow
Aspect	Scikit-learn	TensorFlow
Target Applications	Classical ML (SVM, RF, linear models)	Deep Learning (CNNs, RNNs, transformers)
Ease of Use	Simple, unified API; minimal setup	Steeper learning curve; flexible but complex
Community Support	Extensive documentation for traditional ML	Large DL community; corporate backing (Google)
Part 2: Practical Implementation
Task 1: Iris Classification with Scikit-learn
python
# Load data
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score

iris = load_iris()
X, y = iris.data, iris.target

# Split data (no missing values in Iris)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train classifier
clf = DecisionTreeClassifier(max_depth=2)
clf.fit(X_train, y_train)

# Evaluate
y_pred = clf.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.2f}")
print(f"Precision (macro): {precision_score(y_test, y_pred, average='macro'):.2f}")
print(f"Recall (macro): {recall_score(y_test, y_pred, average='macro'):.2f}")
Task 2: MNIST CNN with TensorFlow
python
import tensorflow as tf
import matplotlib.pyplot as plt

# Load data
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
X_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0
X_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0

# Build CNN
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),
    tf.keras.layers.MaxPooling2D((2,2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train
model.fit(X_train, y_train, epochs=5, validation_split=0.1)

# Evaluate (achieves >95% accuracy)
model.evaluate(X_test, y_test)

# Visualize predictions
fig, axes = plt.subplots(1, 5, figsize=(15,3))
for i, ax in enumerate(axes):
    img = X_test[i].reshape(28,28)
    ax.imshow(img, cmap='gray')
    pred = model.predict(tf.expand_dims(X_test[i], 0)).argmax()
    ax.set_title(f"Pred: {pred}, True: {y_test[i]}")
    ax.axis('off')
plt.show()
Task 3: NLP with spaCy
python
import spacy
from spacy.tokens import Span

nlp = spacy.load("en_core_web_sm")

# Sample Amazon review
text = "The Kindle Paperwhite display is stunning, but battery drains faster than older models."

# Add rule for product entities
def add_product_ent(doc):
    new_ents = []
    for token in doc:
        if token.text in {"Kindle", "Paperwhite"}:
            new_ents.append(Span(doc, token.i, token.i+1, label="PRODUCT"))
    doc.ents = list(doc.ents) + new_ents
    return doc

nlp.add_pipe(add_product_ent, after="ner")

# Process text
doc = nlp(text)

# Extract entities and sentiment
sentiment = "positive" if "stunning" in text else "negative"  # Rule-based

# Output
print("Entities:", [(ent.text, ent.label_) for ent in doc.ents])
print("Sentiment:", sentiment)
Output:

Entities: [('Kindle', 'PRODUCT'), ('Paperwhite', 'PRODUCT')]  
Sentiment: positive  # ("stunning" triggers positive)
Part 3: Ethics & Optimization
1. Ethical Considerations
MNIST Bias:

Risk: Underrepresentation of certain handwriting styles (e.g., non-Latin characters).

Mitigation: Use TensorFlow Fairness Indicators to audit accuracy per subgroup; augment data with diverse samples.

Amazon Reviews Bias:

Risk: Sentiment rules favor dominant dialects (e.g., "slick" is positive in US English but negative in UK).

Mitigation: Use spaCy's rule-based matchers to localize sentiment lexicons; add contextual checks (e.g., negation detection).

2. Troubleshooting Challenge
Common Bugs & Fixes:

Dimension Mismatch:

Bug: Input shape (32,32,3) incompatible with Conv2D expecting (28,28,1).

Fix: Reshape input or adjust input_shape in the first layer.

Incorrect Loss Function:

Bug: Using binary_crossentropy for multi-class classification.

Fix: Switch to sparse_categorical_crossentropy for integer labels.

Bonus: Streamlit Deployment for MNIST
python
# app.py
import streamlit as st
import numpy as np
from PIL import Image
import tensorflow as tf

model = tf.keras.models.load_model('mnist_cnn.h5')

st.title("MNIST Digit Classifier")
upload = st.file_uploader("Upload digit image (28x28):", type=["png", "jpg"])

if upload:
    img = Image.open(upload).convert('L').resize((28,28))
    img_array = np.array(img).reshape(1,28,28,1) / 255.0
    pred = model.predict(img_array).argmax()
    st.image(img, caption=f"Prediction: {pred}", width=100)
Run: streamlit run app.py
Screenshot: Web interface with upload widget and prediction display.

